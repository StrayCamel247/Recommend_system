{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improved by word2vec "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 改进思路：1\n",
    "## 加入停用词，blurb中的简介利用word2vec训练的向量替代，效果比原版提升了10%， 在15代的时候达到最好。loss=10.7, val_loss=11.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 改进思路：2\n",
    "## 改变模型架构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys.version_info(major=3, minor=7, micro=6, releaselevel='final', serial=0)\n",
      "tensorflow 2.0.0\n",
      "matplotlib 3.1.1\n",
      "numpy 1.16.4\n",
      "pandas 0.25.3\n",
      "sklearn 0.22.1\n",
      "tensorflow 2.0.0\n",
      "tensorflow_core.keras 2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "import os, sys, re, pickle\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "import math\n",
    "from nltk.corpus import stopwords\n",
    "print(sys.version_info)\n",
    "for module in tf, mpl, np, pd, sklearn, tf, keras:\n",
    "    print(module.__name__, module.__version__)\n",
    "\n",
    "from Data import DataLoad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_DATA = DataLoad()\n",
    "origin_DATA.features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "blurb_embedding_dim = 50\n",
    "def trainWord2Vec(size=blurb_embedding_dim):#训练word2vec模型并存储\n",
    "    train_sentences = blurb_word_list\n",
    "    # 这里min_count会对字典做截断，如果min_coun=5,那么出现次数小于5的词会被丢弃，需要更新原本的字典\n",
    "    model=Word2Vec(sentences=train_sentences,size=size,min_count=0,window=5)\n",
    "    model.save('./blurb_word2vec.model')\n",
    "trainWord2Vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159655, 50)\n",
      "(159656, 50)\n"
     ]
    }
   ],
   "source": [
    "def load_word2vec_martics(path = './blurb_word2vec.model', size=blurb_embedding_dim):\n",
    "    # 得到向量矩阵\n",
    "    model=Word2Vec.load(path)\n",
    "    blurb_word2vec_martics = np.zeros((len(blurb_dict), size))\n",
    "    for i, value in blurb_dict.items():\n",
    "        try:\n",
    "            blurb_word2vec_martics[value, :] = model.wv[i]\n",
    "        except:\n",
    "            print(i ,value)\n",
    "            print(model.wv[i])\n",
    "            exit(1)\n",
    "    print(blurb_word2vec_martics.shape)\n",
    "    # 添加pad字符对应的向量\n",
    "    blurb_word2vec_martics = np.insert(blurb_word2vec_martics, blurb_word2vec_martics.shape[0], axis=0, values=np.zeros(shape=(1, size)))\n",
    "    print(blurb_word2vec_martics.shape)\n",
    "    return blurb_word2vec_martics\n",
    "\n",
    "blurb_word2vec_martics = load_word2vec_martics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172102\n",
      "172102\n"
     ]
    }
   ],
   "source": [
    "# 增加一个pad字符\n",
    "print(len(blurb_word_list))\n",
    "blurb_dict['<padd>'] = len(blurb_dict)\n",
    "print(len(blurb_word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pad_index =  159655\n",
      "(172102, 200)\n"
     ]
    }
   ],
   "source": [
    "def pre_blurb():\n",
    "    # 将blurb转换为数字矩阵，并处理长度\n",
    "    pad_index = blurb_dict['<padd>']\n",
    "    print('pad_index = ', pad_index)\n",
    "    blurb_int_list = []\n",
    "    for one_blurb in blurb_word_list:\n",
    "        # 补齐和去长\n",
    "        one_blurb = one_blurb[:200]\n",
    "        if len(one_blurb)< 200:\n",
    "            one_blurb = one_blurb + ['<padd>'] * (200- len(one_blurb))\n",
    "\n",
    "        # 转化为数字列表\n",
    "        temp_list = []\n",
    "        for word in one_blurb:\n",
    "            temp_list.append(blurb_dict[word])\n",
    "        blurb_int_list.append(temp_list)\n",
    "\n",
    "    blurb = np.array(blurb_int_list)\n",
    "    print(blurb.shape)\n",
    "    return blurb\n",
    "\n",
    "blurb = pre_blurb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all user id =  28836\n",
      "all location =  7573\n"
     ]
    }
   ],
   "source": [
    "# user-id的字典,总共有28836个用户\n",
    "all_user = len(set(origin_DATA.features['User-ID']))\n",
    "new_user_id = {val: i for i, val in enumerate(set(origin_DATA.features['User-ID']))}\n",
    "print('all user id = ', all_user)\n",
    "# location的数量=7573(从0开始的)\n",
    "all_location = max([j for i in origin_DATA.features.Location for j in i]) +1 \n",
    "print('all location = ', all_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all isbn =  38036\n",
      "all author =  15196\n",
      "all year =  81\n",
      "all publisher =  2909\n",
      "all title words =  23815\n",
      "all blurb words =  159656\n"
     ]
    }
   ],
   "source": [
    "# ISBN总数\n",
    "all_isbn = len(set(origin_DATA.features['ISBN']))\n",
    "print('all isbn = ', all_isbn)\n",
    "# author总数\n",
    "all_author = len(set(origin_DATA.features['Author']))\n",
    "print('all author = ', all_author)\n",
    "# year总数\n",
    "all_year = len(set(origin_DATA.features['Year']))\n",
    "print('all year = ', all_year)\n",
    "# publish总数\n",
    "all_publisher = len(set(origin_DATA.features['Publisher']))\n",
    "print('all publisher = ', all_publisher)\n",
    "# title中所有单词总数\n",
    "all_title_words = max([j for i in origin_DATA.features.Title for j in i]) +1\n",
    "print('all title words = ', all_title_words)\n",
    "# blurb中所有单词总数\n",
    "all_blurb_words = len(blurb_dict)\n",
    "print('all blurb words = ', all_blurb_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_title(all_title_words=all_title_words):\n",
    "    # 将title转换为数字矩阵，并处理长度\n",
    "    # 对title进行补齐\n",
    "    title = []\n",
    "    for ti in origin_DATA.features['Title'].values:\n",
    "        if len(ti) > 10:\n",
    "            ti = ti[:10]\n",
    "        if len(ti) < 10:\n",
    "            ti = ti + [all_title_words] * (10 - len(ti))\n",
    "\n",
    "        title.append(ti)\n",
    "    all_title_words += 1\n",
    "    title = np.array(title)\n",
    "    return title, all_title_words\n",
    "\n",
    "title,all_title_words = pre_title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_val_test(): \n",
    "    m = len(origin_DATA.features['Location'])\n",
    "    # 对location取3位数\n",
    "    loca = np.zeros((m, 3))\n",
    "    for i in range(m):\n",
    "        loca[i] = np.array(origin_DATA.features['Location'][i])\n",
    "\n",
    "    input_features = [origin_DATA.features['User-ID'].to_numpy(), loca, \n",
    "                      origin_DATA.features['ISBN'].to_numpy(), origin_DATA.features['Author'].to_numpy(),\n",
    "                     origin_DATA.features['Year'].to_numpy(), origin_DATA.features['Publisher'].to_numpy(), \n",
    "                     title, blurb]\n",
    "    labels = origin_DATA.labels.to_numpy()\n",
    "    # 分割数据集以及shuffle\n",
    "    np.random.seed(100)\n",
    "    number_features = len(input_features)\n",
    "    shuffle_index = np.random.permutation(m)\n",
    "    shuffle_train_index = shuffle_index[:math.ceil(m*0.96)]\n",
    "    shuffle_val_index = shuffle_index[math.ceil(m*0.96): math.ceil(m*0.98)]\n",
    "    shuffle_test_index = shuffle_index[math.ceil(m*0.98):]\n",
    "    train_features = [input_features[i][shuffle_train_index] for i in range(number_features)]\n",
    "    train_labels = labels[shuffle_train_index]\n",
    "    val_features = [input_features[i][shuffle_val_index] for i in range(number_features)]\n",
    "    val_lables = labels[shuffle_val_index]\n",
    "    test_features = [input_features[i][shuffle_test_index] for i in range(number_features)]\n",
    "    test_lables = labels[shuffle_test_index]\n",
    "    return train_features, train_labels, val_features, val_lables, test_features, test_lables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(165218,)\n",
      "(3442,)\n",
      "(3442,)\n"
     ]
    }
   ],
   "source": [
    "train_features, train_labels, val_features, val_lables, test_features, test_lables = get_train_val_test()\n",
    "print(train_features[0].shape)\n",
    "print(val_features[0].shape)\n",
    "print(test_features[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inputs():\n",
    "    # 用户特征输入\n",
    "    user_id = keras.layers.Input(shape=(1,), dtype='int32', name='user_id_input')\n",
    "    user_location = keras.layers.Input(shape=(3,), dtype='int32', name='user_location_input')\n",
    "    \n",
    "    # 书籍特征输入\n",
    "    book_isbn = keras.layers.Input(shape=(1,),  dtype='int32', name='book_isbn_input')\n",
    "    book_author = keras.layers.Input(shape=(1,),  dtype='int32', name='book_author_input')\n",
    "    book_year = keras.layers.Input(shape=(1,),  dtype='int32', name='book_year_input')\n",
    "    book_publisher = keras.layers.Input(shape=(1,),  dtype='int32', name='book_publisher_input')  \n",
    "    book_title = keras.layers.Input(shape=(10, ), dtype='int32', name='book_title_input')\n",
    "    book_blurb = keras.layers.Input(shape=(200, ), dtype='int32', name='book_blurb_input')\n",
    "    return user_id, user_location, book_isbn, book_author, book_year, book_publisher, book_title, book_blurb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 嵌入矩阵的维度\n",
    "embed_dim = 4\n",
    "embed_dim_title = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_embed_layer(u_id, u_loca):\n",
    "    user_id_embedd = keras.layers.Embedding(all_user, embed_dim, name='user_id_embedding')(u_id)\n",
    "    user_loca_embedd = keras.layers.Embedding(all_location, embed_dim , name='user_loca_embedding')(u_loca)\n",
    "    return user_id_embedd, user_loca_embedd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def book_emded_layer(b_isbn, b_atuhor, b_year, b_publisher, b_title, b_blurb):\n",
    "    book_isbn_embedd = keras.layers.Embedding(all_isbn, embed_dim, name='book_isbn_embedding')(b_isbn)\n",
    "    book_author_embedd = keras.layers.Embedding(all_author, embed_dim, name='book_author_embedding')(b_atuhor)\n",
    "    book_year_embedd = keras.layers.Embedding(all_year, embed_dim, name='book_year_embedding')(b_year)\n",
    "    book_publisher_embedd = keras.layers.Embedding(all_publisher, embed_dim, name='book_publisher_embedding')(b_publisher)\n",
    "    \n",
    "    book_title_embedd = keras.layers.Embedding(all_title_words, embed_dim_title, name='book_title_embedding')(b_title)\n",
    "    # 加载预训练模型\n",
    "    book_blurb_embedd = keras.layers.Embedding(all_blurb_words, blurb_embedding_dim, name='book_blurb_embedding', weights=[blurb_word2vec_martics], trainable=True)(b_blurb)\n",
    "    return book_isbn_embedd, book_author_embedd, book_year_embedd, book_publisher_embedd, book_title_embedd, book_blurb_embedd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_feature(u_id_embedd, u_loca_embedd):\n",
    "    u_id_layer = keras.layers.Dense(32, activation='relu', kernel_regularizer=tf.nn.l2_loss, name='u_id_dense')(u_id_embedd)\n",
    "#     u_id_layer_drop = keras.layers.Dropout(rate=0.5, name='u_id_layer_drop')(u_id_layer)\n",
    "    # u_id_layer.shape = (?, 1, 32)\n",
    "    # u_loca_layer.shape = (?, 32)\n",
    "    # 这里可以再加个Dense\n",
    "    u_loca_layer = keras.layers.Bidirectional(tf.keras.layers.LSTM(16, name='u_loca_bilstm'), merge_mode='concat')(u_loca_embedd)\n",
    "    u_loca_lstm_dense = keras.layers.Dense(32, activation='relu', kernel_regularizer=tf.nn.l2_loss, name='u_loca_lstm_dense')(u_loca_layer)\n",
    "    u_id_reshape = keras.layers.Reshape([32])(u_id_layer)\n",
    "    u_combine = keras.layers.concatenate([u_id_reshape, u_loca_lstm_dense],axis=1, name='u_combine')\n",
    "    print(u_combine.shape)\n",
    "    # 这里能不能用激活函数\n",
    "    u_feature_layer = keras.layers.Dense(100, activation='tanh', name='u_feature_layer')(u_combine)\n",
    "    print(u_feature_layer.shape)\n",
    "    return u_feature_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_dense = 4\n",
    "def get_book_feature(b_isbn_embedd, b_author_embedd, b_year_embedd, b_publisher_embedd, b_title_embedd, b_blurb_embedd):\n",
    "    # 首先对前4个特征连接Dense层\n",
    "    b_isbn_dense = keras.layers.Dense(b_dense, activation='relu', kernel_regularizer=tf.nn.l2_loss, name='b_isbn_dense')(b_isbn_embedd)\n",
    "    b_author_dense = keras.layers.Dense(b_dense, activation='relu', kernel_regularizer=tf.nn.l2_loss, name='b_author_dense')(b_author_embedd)\n",
    "    b_year_dense = keras.layers.Dense(b_dense, activation='relu', kernel_regularizer=tf.nn.l2_loss, name='b_year_dense')(b_year_embedd)\n",
    "    b_publisher_dense = keras.layers.Dense(b_dense, activation='relu', kernel_regularizer=tf.nn.l2_loss, name='b_publisher_dense')(b_publisher_embedd)\n",
    "    # 合并这四个特征,  b_combine_four shape = (?, 1, 16)\n",
    "    b_combine_four = keras.layers.concatenate([b_isbn_dense, b_author_dense, b_year_dense, b_publisher_dense], name='b_four_combine')\n",
    "    print('b_combine_four.shape', b_combine_four.shape)\n",
    "    # 对title进行卷积\n",
    "    b_title_reshape = keras.layers.Lambda(lambda layer: tf.expand_dims(layer, 3))(b_title_embedd)  # shape=(?,10, 16, 1)\n",
    "    print('b_title_reshape.shape = ', b_title_reshape.shape)\n",
    "    # b_title_conv.shape = \n",
    "    b_title_conv = keras.layers.Conv2D(filters=8, kernel_size=(2, embed_dim_title), kernel_regularizer=tf.nn.l2_loss, strides=1)(b_title_reshape)# shape=(?, 14, 1, 8)\n",
    "    # b-title_pool.shape =\n",
    "    b_title_pool = keras.layers.MaxPool2D(pool_size=(9, 1), strides=1)(b_title_conv) # shape=(?,1, 1, 8)\n",
    "    print('b_title_conv.shape = ', b_title_conv)\n",
    "    print('b_title_pool.shape = ', b_title_pool)\n",
    "    \n",
    "    # 对blurb进行处理\n",
    "    # shape = \n",
    "    b_blurb_lstm_1 = keras.layers.Bidirectional(tf.keras.layers.LSTM(16, name='b_blurb_bilstm', dropout=0.5, return_sequences=True), merge_mode='concat')(b_blurb_embedd) \n",
    "    print('b_blurb_lstm_1.shape = ', b_blurb_lstm_1.shape)\n",
    "    b_blurb_lstm_2 = keras.layers.LSTM(32, name='b_blurb_lstm', dropout=0.5, return_sequences=False)(b_blurb_lstm_1) \n",
    "    print('b_blurb_lstm_2.shape = ', b_blurb_lstm_2.shape)\n",
    "    # 将title和blurb合并\n",
    "    b_title_reshape = keras.layers.Reshape([b_title_pool.shape[3]])(b_title_pool)\n",
    "    # b_combine_blurb_title.shape = \n",
    "    b_combine_blurb_title = keras.layers.concatenate([b_title_reshape, b_blurb_lstm_2], axis=1, name='b_combine_blurb_title')\n",
    "    print('b_combine_blurb_title.shape = ', b_combine_blurb_title)\n",
    "    b_blurb_title_dense = keras.layers.Dense(64, kernel_regularizer=tf.nn.l2_loss, activation='relu', name='b_blurb_title_dense')(b_combine_blurb_title)\n",
    "#     b_blurb_title_dense_drop = keras.layers.Dropout(rate=0.5, name='b_blurb_title_dense_drop')(b_blurb_title_dense)\n",
    "    b_blurb_title_dense = keras.layers.Dense(64, activation='relu', name='b_blurb_title_dense')(b_combine_blurb_title)\n",
    "    # b_combine_four_reshape shape = (?, 64)\n",
    "    b_combine_four_reshape = keras.layers.Reshape([b_combine_four.shape[2]], name='b_combine_four_reshape')(b_combine_four)\n",
    "    # 合并所有的书籍特征\n",
    "#     b_combine_book = keras.layers.concatenate([b_combine_blurb_title, b_combine_four_reshape], axis=1, name='b_combine_book')\n",
    "    b_combine_book = keras.layers.concatenate([b_blurb_title_dense, b_combine_four_reshape], axis=1, name='b_combine_book')\n",
    "\n",
    "    # 得到书籍矩阵\n",
    "    b_feature_layer = keras.layers.Dense(100, name='b_feature_layer', activation='tanh')(b_combine_book)\n",
    "    return b_feature_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rating(user_feature, book_feature):\n",
    "#     multiply_layer = keras.layers.Lambda(lambda layer: tf.reduce_sum(layer[0]+layer[1], axis=1, keepdims=True), name = 'user_book_feature')((user_feature, book_feature))\n",
    "    inference_layer = keras.layers.concatenate([user_feature, book_feature], axis=1, name='user_book_feature')\n",
    "    inference_dense = tf.keras.layers.Dense(64, kernel_regularizer=tf.nn.l2_loss, activation='relu')(inference_layer)\n",
    "    multiply_layer = tf.keras.layers.Dense(1, name=\"inference\")(inference_layer)  # inference_dense\n",
    "    print(multiply_layer.shape)\n",
    "    return multiply_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = './model/'\n",
    "\n",
    "class model_network():\n",
    "    def __init__(self):\n",
    "        self.batchsize = 512\n",
    "        self.epoch = 15\n",
    "    def creat_model(self):\n",
    "        user_id, user_location, book_isbn, book_author, book_year, book_publisher, book_title, book_blurb = get_inputs()\n",
    "        user_id_embedd, user_loca_embedd = user_embed_layer(user_id, user_location)\n",
    "        book_isbn_embedd, book_author_embedd, book_year_embedd, book_publisher_embedd, book_title_embedd, book_blurb_embedd = book_emded_layer(book_isbn, book_author, book_year, book_publisher, book_title, book_blurb)\n",
    "        u_feature_layer = get_user_feature(user_id_embedd, user_loca_embedd)\n",
    "        b_feature_layer = get_book_feature(book_isbn_embedd, book_author_embedd, book_year_embedd, book_publisher_embedd, book_title_embedd, book_blurb_embedd)\n",
    "        multiply_layer = get_rating(u_feature_layer, b_feature_layer)\n",
    "        model = keras.Model(inputs=[user_id, user_location, book_isbn, book_author, book_year, book_publisher, book_title, book_blurb],\n",
    "                    outputs=[multiply_layer])\n",
    "        return model\n",
    "    def train_model(self):\n",
    "        weights_path = './model_weights/model_2.hdf5'\n",
    "#         checkpoint = keras.callbacks.ModelCheckpoint(filepath=weights_path, monitor='val_loss', mode='min', save_weights_only=True)\n",
    "        model_optimizer = keras.optimizers.Adamax()\n",
    "        model = self.creat_model()\n",
    "        model.compile(optimizer=model_optimizer, loss=keras.losses.mse)\n",
    "        history = model.fit(train_features, train_labels, validation_data=(val_features, val_lables), epochs=self.epoch, batch_size=self.batchsize, verbose=1)\n",
    "        print(model.summary())\n",
    "        return model, history\n",
    "    def predict_model(self, model):\n",
    "        test_loss = model.evaluate(test_features, test_lables, verbose=0)\n",
    "        return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 64)\n",
      "(None, 100)\n",
      "b_combine_four.shape (None, 1, 16)\n",
      "b_title_reshape.shape =  (None, 10, 16, 1)\n",
      "b_title_conv.shape =  Tensor(\"conv2d/Identity:0\", shape=(None, 9, 1, 8), dtype=float32)\n",
      "b_title_pool.shape =  Tensor(\"max_pooling2d/Identity:0\", shape=(None, 1, 1, 8), dtype=float32)\n",
      "b_blurb_lstm_1.shape =  (None, 200, 32)\n",
      "b_blurb_lstm_2.shape =  (None, 32)\n",
      "b_combine_blurb_title.shape =  Tensor(\"b_combine_blurb_title/Identity:0\", shape=(None, 40), dtype=float32)\n",
      "(None, 1)\n",
      "Train on 165218 samples, validate on 3442 samples\n",
      "Epoch 1/15\n",
      "165218/165218 [==============================] - 43s 261us/sample - loss: 26.9403 - val_loss: 17.4714\n",
      "Epoch 2/15\n",
      "165218/165218 [==============================] - 33s 197us/sample - loss: 15.4060 - val_loss: 13.3318\n",
      "Epoch 3/15\n",
      "165218/165218 [==============================] - 33s 199us/sample - loss: 13.2912 - val_loss: 12.4378\n",
      "Epoch 4/15\n",
      "165218/165218 [==============================] - 33s 200us/sample - loss: 12.7005 - val_loss: 12.1677\n",
      "Epoch 5/15\n",
      "103424/165218 [=================>............] - ETA: 12s - loss: 12.4615- ETA: 16s - lo - ETA: 12s - loss: 12. - ETA: 12s - loss: 12.4623"
     ]
    }
   ],
   "source": [
    "net_work = model_network()\n",
    "model, history = net_work.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "plt.figure(1)\n",
    "plt.plot(train_loss, c='r', label='train_loss')\n",
    "plt.plot(val_loss, c='b', label='val_loss')\n",
    "plt.legend()\n",
    "plt.xlim([0, 15])\n",
    "# plt.ylim([0, 20])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = net_work.predict_model(model)\n",
    "print(test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 画出模型图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.plot_model(model, to_file='model_2.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}