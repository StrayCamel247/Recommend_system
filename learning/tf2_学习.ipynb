{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 包的导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "sys.version_info(major=3, minor=6, micro=7, releaselevel='final', serial=0)\ntensorflow 2.1.0\nmatplotlib 3.2.0\nnumpy 1.18.1\npandas 1.0.1\nsklearn 0.22.2.post1\ntensorflow 2.1.0\ntensorflow_core.python.keras.api._v2.keras 2.2.4-tf\n"
    }
   ],
   "source": [
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "import os, sys, time \n",
    "\n",
    "\n",
    "print(sys.version_info)\n",
    "for module in tf, mpl, np, pd, sklearn, tf, keras:\n",
    "    print(module.__name__, module.__version__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF data模块处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "<TensorSliceDataset shapes: (), types: tf.float32>\ntf.Tensor(0.0, shape=(), dtype=float32)\ntf.Tensor(1.0, shape=(), dtype=float32)\ntf.Tensor(2.0, shape=(), dtype=float32)\ntf.Tensor(3.0, shape=(), dtype=float32)\ntf.Tensor(4.0, shape=(), dtype=float32)\ntf.Tensor(5.0, shape=(), dtype=float32)\ntf.Tensor(6.0, shape=(), dtype=float32)\ntf.Tensor(7.0, shape=(), dtype=float32)\ntf.Tensor(8.0, shape=(), dtype=float32)\ntf.Tensor(9.0, shape=(), dtype=float32)\n"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(np.arange(10,dtype=np.float32))\n",
    "print(dataset)\n",
    "for _ in dataset:\n",
    "    print(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tf.Tensor([0. 1. 2. 3. 4. 5. 6.], shape=(7,), dtype=float32)\ntf.Tensor([7. 8. 9. 0. 1. 2. 3.], shape=(7,), dtype=float32)\ntf.Tensor([4. 5. 6. 7. 8. 9. 0.], shape=(7,), dtype=float32)\ntf.Tensor([1. 2. 3. 4. 5. 6. 7.], shape=(7,), dtype=float32)\ntf.Tensor([8. 9.], shape=(2,), dtype=float32)\n"
    }
   ],
   "source": [
    "'''\n",
    "数据操作：\n",
    "'''\n",
    "# repeat epoch+get batch\n",
    "dataset = dataset.repeat(3).batch(7)\n",
    "for _ in dataset:\n",
    "    print(_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tf.Tensor(0.0, shape=(), dtype=float32)\ntf.Tensor(1.0, shape=(), dtype=float32)\ntf.Tensor(2.0, shape=(), dtype=float32)\ntf.Tensor(3.0, shape=(), dtype=float32)\ntf.Tensor(4.0, shape=(), dtype=float32)\ntf.Tensor(7.0, shape=(), dtype=float32)\ntf.Tensor(8.0, shape=(), dtype=float32)\ntf.Tensor(9.0, shape=(), dtype=float32)\ntf.Tensor(0.0, shape=(), dtype=float32)\ntf.Tensor(1.0, shape=(), dtype=float32)\ntf.Tensor(4.0, shape=(), dtype=float32)\ntf.Tensor(5.0, shape=(), dtype=float32)\ntf.Tensor(6.0, shape=(), dtype=float32)\ntf.Tensor(7.0, shape=(), dtype=float32)\ntf.Tensor(8.0, shape=(), dtype=float32)\ntf.Tensor(1.0, shape=(), dtype=float32)\ntf.Tensor(2.0, shape=(), dtype=float32)\ntf.Tensor(3.0, shape=(), dtype=float32)\ntf.Tensor(4.0, shape=(), dtype=float32)\ntf.Tensor(5.0, shape=(), dtype=float32)\ntf.Tensor(8.0, shape=(), dtype=float32)\ntf.Tensor(9.0, shape=(), dtype=float32)\ntf.Tensor(5.0, shape=(), dtype=float32)\ntf.Tensor(6.0, shape=(), dtype=float32)\ntf.Tensor(2.0, shape=(), dtype=float32)\ntf.Tensor(3.0, shape=(), dtype=float32)\ntf.Tensor(9.0, shape=(), dtype=float32)\ntf.Tensor(0.0, shape=(), dtype=float32)\ntf.Tensor(6.0, shape=(), dtype=float32)\ntf.Tensor(7.0, shape=(), dtype=float32)\n"
    }
   ],
   "source": [
    "# interleave\n",
    "# case ：文件dataset -> 具体数据集\n",
    "dataset2 = dataset.interleave(\n",
    "    # map_fn\n",
    "    lambda x: tf.data.Dataset.from_tensor_slices(x),\n",
    "    # cycle_length\n",
    "    cycle_length = 5,\n",
    "    # block_length\n",
    "    block_length = 5,\n",
    ")\n",
    "for _ in dataset2:\n",
    "    print(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "value: \"machine learning\"\nvalue: \"cc150\"\n\nvalue: 15.5\nvalue: 9.5\nvalue: 7.0\nvalue: 8.0\n\nvalue: 42\n\nfeature {\n  key: \"age\"\n  value {\n    int64_list {\n      value: 42\n    }\n  }\n}\nfeature {\n  key: \"favorite_book\"\n  value {\n    bytes_list {\n      value: \"machine learning\"\n      value: \"cc150\"\n    }\n  }\n}\nfeature {\n  key: \"hours\"\n  value {\n    float_list {\n      value: 15.5\n      value: 9.5\n      value: 7.0\n      value: 8.0\n    }\n  }\n}\n\n"
    }
   ],
   "source": [
    "# tfrecord 文件格式\n",
    "# -> tf.train.Example\n",
    "# -> tf.train.Feature -> {'key':tf.train.Feature}\n",
    "# -> tf.train.Feature - >tf.train.ByteList/FloatList/Int64List\n",
    "favorite_books = [name.encode('utf-8') for name in ['machine learning', 'cc150']]\n",
    "favorite_books_bytelist = tf.train.BytesList(value=favorite_books)\n",
    "print(favorite_books_bytelist)\n",
    "\n",
    "hours_floatlist = tf.train.FloatList(value=[15.5, 9.5, 7.0, 8.0])\n",
    "print(hours_floatlist)\n",
    "\n",
    "age_int64list = tf.train.Int64List(value=[42])\n",
    "print(age_int64list)\n",
    "\n",
    "features = tf.train.Features(\n",
    "    feature = {\n",
    "        'favorite_book': tf.train.Feature(bytes_list = favorite_books_bytelist),\n",
    "        'hours': tf.train.Feature(float_list = hours_floatlist),\n",
    "        'age': tf.train.Feature(int64_list = age_int64list),\n",
    "    }\n",
    ")\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF 骚操作\n",
    "将自己写的function转化成tf.function\n",
    "优势：快"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tf.Tensor(-0.63212055, shape=(), dtype=float32)\ntf.Tensor([-0.95021296 -0.917915  ], shape=(2,), dtype=float32)\ntf.Tensor(-0.63212055, shape=(), dtype=float32)\ntf.Tensor([-0.95021296 -0.917915  ], shape=(2,), dtype=float32)\ntf.Tensor(-0.63212055, shape=(), dtype=float32)\ntf.Tensor([-0.95021296 -0.917915  ], shape=(2,), dtype=float32)\n"
    },
    {
     "data": {
      "text/markdown": "```python\ndef tf__scaled_elu(z, scale=None, alpha=None):\n  do_return = False\n  retval_ = ag__.UndefinedReturnValue()\n  with ag__.FunctionScope('scaled_elu', 'fscope', ag__.ConversionOptions(recursive=True, user_requested=True, optional_features=(), internal_convert_user_code=True)) as fscope:\n    is_postive = ag__.converted_call(tf.greater_equal, (z, 0.0), None, fscope)\n    do_return = True\n    retval_ = fscope.mark_return_value(scale * ag__.converted_call(tf.where, (is_postive, z, alpha * ag__.converted_call(tf.nn.elu, (z,), None, fscope)), None, fscope))\n  do_return,\n  return ag__.retval(retval_)\n\n```",
      "text/plain": "<IPython.core.display.Markdown object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tf.function and auto-graph\n",
    "def scaled_elu(z, scale=1.0, alpha=1.0):\n",
    "    # z>=0? scale * z : scale:alpha * tf.nn.eu(z)\n",
    "    is_postive = tf.greater_equal(z, 0.0)\n",
    "    return scale * tf.where(is_postive, z, alpha * tf.nn.elu(z))\n",
    "print(scaled_elu(tf.constant(-1.)))\n",
    "print(scaled_elu(tf.constant([-3., -2.5])))\n",
    "# 用tf.function 将函数转化为tf函数\n",
    "# 方式1\n",
    "tf_scale = tf.function(scaled_elu)\n",
    "print(tf_scale(tf.constant(-1.)))\n",
    "print(tf_scale(tf.constant([-3., -2.5])))\n",
    "# 方式2\n",
    "@tf.function\n",
    "def tf_scale2(z, scale=1.0, alpha=1.0):\n",
    "    # z>=0? scale * z : scale:alpha * tf.nn.eu(z)\n",
    "    is_postive = tf.greater_equal(z, 0.0)\n",
    "    return scale * tf.where(is_postive, z, alpha * tf.nn.elu(z))\n",
    "print(tf_scale2(tf.constant(-1.)))\n",
    "print(tf_scale2(tf.constant([-3., -2.5])))\n",
    "\n",
    "# 将展转化后的函数展示出来\n",
    "def display_tf_code(func):\n",
    "    code = tf.autograph.to_code(func)\n",
    "    from IPython.display import display, Markdown\n",
    "    display(Markdown('```python\\n{}\\n```'.format(code)))\n",
    "# 展示源码\n",
    "display_tf_code(scaled_elu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tf.Tensor(21.0, shape=(), dtype=float32)\n"
    }
   ],
   "source": [
    "var = tf.Variable(0.)\n",
    "\n",
    "@tf.function\n",
    "def add_21():\n",
    "    return var.assign_add(21)\n",
    "print(add_21())\n",
    "\n",
    "# 如果将var放在函数里面?,会报错!所以在我们定义使用function之前我们需要将var = tf.Variable(0.)定义在函数初始化之前\n",
    "@tf.function\n",
    "def add_21():\n",
    "    var = tf.Variable(0.)\n",
    "    return var.assign_add(21)\n",
    "# print(add_21()) 打印会报错\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tf.Tensor([ 1.  8. 27.], shape=(3,), dtype=float32)\ntf.Tensor([ 1  8 27], shape=(3,), dtype=int32)\n"
    }
   ],
   "source": [
    "@tf.function()\n",
    "def cube(z):\n",
    "    return tf.pow(z,3)\n",
    "# 我们输入的数据类型不一样 输出的类型也不一样\n",
    "print(cube(tf.constant([1.,2.,3.])))\n",
    "print(cube(tf.constant([1,2,3])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Python inputs incompatible with input_signature:\n  inputs: (\n    tf.Tensor([1. 2. 3.], shape=(3,), dtype=float32))\n  input_signature: (\n    TensorSpec(shape=(None,), dtype=tf.int32, name='x'))\n"
    }
   ],
   "source": [
    "# 我们给函数加上一个变量限制\n",
    "@tf.function(input_signature=[tf.TensorSpec([None], tf.int32, name='x')])\n",
    "def cube(z):\n",
    "    return tf.pow(z,3)\n",
    "try:\n",
    "    print(cube(tf.constant([1.,2.,3.])))\n",
    "    print(cube(tf.constant([1,2,3])))\n",
    "except ValueError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "<tensorflow.python.eager.function.ConcreteFunction object at 0x0000021D901E6EF0>\nTrue\nTrue\nFuncGraph(name=cube, id=2325995326824)\n[<tf.Operation 'x' type=Placeholder>, <tf.Operation 'Pow/y' type=Const>, <tf.Operation 'Pow' type=Pow>, <tf.Operation 'Identity' type=Identity>]\n"
    }
   ],
   "source": [
    "cube_func_int32 = cube.get_concrete_function(tf.TensorSpec([None],tf.int32, name='x'))\n",
    "print(cube_func_int32)\n",
    "print(cube_func_int32 is cube.get_concrete_function(tf.TensorSpec([5], tf.int32, name='x')))\n",
    "print(cube_func_int32 is cube.get_concrete_function(tf.constant([1,2,3])))\n",
    "print(cube_func_int32.graph)\n",
    "print(cube_func_int32.graph.get_operations())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "name: \"Pow\"\nop: \"Pow\"\ninput: \"x\"\ninput: \"Pow/y\"\nattr {\n  key: \"T\"\n  value {\n    type: DT_INT32\n  }\n}\n\nPow\n[<tf.Tensor 'x:0' shape=(None,) dtype=int32>, <tf.Tensor 'Pow/y:0' shape=() dtype=int32>]\n[<tf.Tensor 'Pow:0' shape=(None,) dtype=int32>]\n"
    }
   ],
   "source": [
    "pow_op =  cube_func_int32.graph.get_operations()[2]\n",
    "# 打印操作的过程\n",
    "print(pow_op)\n",
    "# 其中的值都是可以取出来的\n",
    "print(pow_op.name)\n",
    "print(list(pow_op.inputs))\n",
    "print(list(pow_op.outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "7.999999999994678\n(8.999999999993236, 47.999999999994714)\n"
    }
   ],
   "source": [
    "# 求近似导数\n",
    "f = lambda x:3. *x **2 +2.*x-1\n",
    "approximate_derivative = lambda f, x, eps=1e-4 : (f(x+eps) - f(x-eps))/(2. * eps)\n",
    "print(approximate_derivative(f,1.))\n",
    "\n",
    "g = lambda x1,x2:(x1+5)*(x2**2)\n",
    "def approximate_gradient(g, x1, x2, eps=1e-3):\n",
    "    dg_x1 = approximate_derivative(lambda x:g(x,x2),x1,eps)\n",
    "    dg_x2 = approximate_derivative(lambda x:g(x2,x),x2,eps)\n",
    "    return dg_x1,dg_x2\n",
    "print(approximate_gradient(g,2.,3.))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tf.Tensor(9.0, shape=(), dtype=float32)\nGradientTape.gradient can only be called once on non-persistent tapes.\n"
    }
   ],
   "source": [
    "x1 = tf.Variable(2.0)\n",
    "x2 = tf.Variable(3.0)\n",
    "with tf.GradientTape() as tape:\n",
    "    z = g(x1,x2)\n",
    "# 对z函数求x1的偏导\n",
    "# tape只能调用一次\n",
    "dz_x1 = tape.gradient(z,x1)\n",
    "print(dz_x1)\n",
    "\n",
    "try:\n",
    "    dz_x2 = tape.gradient(z,x2)\n",
    "except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tf.Tensor(9.0, shape=(), dtype=float32)\ntf.Tensor(42.0, shape=(), dtype=float32)\n"
    }
   ],
   "source": [
    "# 我们加上persistent后就可以求两次偏导了，结果为9和42，和上面的结果误差比较大\n",
    "with tf.GradientTape(persistent = True) as tape:\n",
    "    z = g(x1,x2)\n",
    "\n",
    "dz_x1 = tape.gradient(z,x1)\n",
    "print(dz_x1)\n",
    "\n",
    "try:\n",
    "    dz_x2 = tape.gradient(z,x2)\n",
    "    print(dz_x2)\n",
    "except RuntimeError as e:\n",
    "    print(e)\n",
    "del(tape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[<tf.Tensor: shape=(), dtype=float32, numpy=9.0>, <tf.Tensor: shape=(), dtype=float32, numpy=42.0>]\n"
    }
   ],
   "source": [
    "# 一次性求出两个偏导\n",
    "with tf.GradientTape() as tape:\n",
    "    z = g(x1,x2)\n",
    "\n",
    "dz = tape.gradient(z,[x1,x2])\n",
    "print(dz)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[<tf.Tensor: shape=(), dtype=float32, numpy=9.0>, <tf.Tensor: shape=(), dtype=float32, numpy=42.0>]\n"
    }
   ],
   "source": [
    "\n",
    "x1 = tf.constant(2.0)\n",
    "x2 = tf.constant(3.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x1)\n",
    "    tape.watch(x2)\n",
    "    z = g(x1,x2)\n",
    "print(tape.gradient(z, [x1,x2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(), dtype=float32, numpy=13.0>"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.Variable(5.)\n",
    "with tf.GradientTape() as tape:\n",
    "    z1 = 3*x\n",
    "    z2 = x**2\n",
    "tape.gradient([z1,z2],x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.0>\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\n"
    }
   ],
   "source": [
    "# 模拟一次梯度下降\n",
    "f = lambda x:3. *x **2 +2.*x-1\n",
    "learning_rate = 0.1\n",
    "x = tf.Variable(0.0)\n",
    "print(x)\n",
    "for _ in range(100):\n",
    "    with tf.GradientTape() as tape:\n",
    "        z = f(x)\n",
    "    dz_dx = tape.gradient(z,x)\n",
    "    # x.assign_sub(learning_rate*dz_dx) 表示x= x-(learning_rate*dz_dx)\n",
    "    x.assign_sub(learning_rate*dz_dx)\n",
    "    # print(x)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.0>\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\n"
    }
   ],
   "source": [
    "# 模拟一次梯度下降\n",
    "f = lambda x:3. *x **2 +2.*x-1\n",
    "learning_rate = 0.1\n",
    "x = tf.Variable(0.0)\n",
    "optimizer = keras.optimizers.SGD(lr = learning_rate)\n",
    "print(x)\n",
    "for _ in range(100):\n",
    "    with tf.GradientTape() as tape:\n",
    "        z = f(x)\n",
    "    dz_dx = tape.gradient(z,x)\n",
    "    optimizer.apply_gradients([(dz_dx,x)])\n",
    "    # print(x)\n",
    "    # x.assign_sub(learning_rate*dz_dx) 表示x= x-(learning_rate*dz_dx)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集的准备\n",
    "Fashion-MNIST是一个替代MNIST手写数字集的图像数据集。 它是由Zalando（一家德国的时尚科技公司）旗下的研究部门提供。其涵盖了来自10种类别的共7万个不同商品的正面图片。Fashion-MNIST的大小、格式和训练集/测试集划分与原始的MNIST完全一致。60000/10000的训练测试数据划分，28x28的灰度图片。你可以直接用它来测试你的机器学习和深度学习算法性能，且不需要改动任何的代码。\n",
    "- 60000张训练图像和对应Label；\n",
    "- 10000张测试图像和对应Label；\n",
    "- 10个类别；\n",
    "- 每张图像28x28的分辨率；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "x_valid.shape, y_valid.shape: (5000, 28, 28) (5000,)\nx_train.shape, y_train.shape (55000, 28, 28) (55000,)\nx_test.shape, y_test.shape (10000, 28, 28) (10000,)\n"
    }
   ],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(x_train_all, y_train_all), (x_test, y_test) = fashion_mnist.load_data()\n",
    "# fashion_mnist 训练集共有60000，将前5000作为验证集，后55000作为训练集\n",
    "x_valid, x_train = x_train_all[:5000], x_train_all[5000:]\n",
    "y_valid, y_train = y_train_all[:5000], y_train_all[5000:]\n",
    "\n",
    "print(\"x_valid.shape, y_valid.shape:\",x_valid.shape, y_valid.shape)\n",
    "print(\"x_train.shape, y_train.shape\",x_train.shape, y_train.shape)\n",
    "print(\"x_test.shape, y_test.shape\",x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集样本展示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据集样本展示\n",
    "def show_single_img(img_arr):\n",
    "    plt.imshow(img_arr, cmap=\"binary\")\n",
    "    plt.show()\n",
    "# show_single_img(x_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对数据进行归一化处理\n",
    "> 定义：把数据经过处理后使之限定在一定的范围内。比如通常限制在区间`[0, 1]`或者`[-1, 1]`\n",
    "\n",
    "常用归一化法：\n",
    "- 最大-最小标准化: $$\\frac{x-x_{min}}{x_{max}-x_{min}}$$\n",
    "- Z-score标准化: $$\\frac{x-\\mu }{std} \\left ( \\mu为标准差 ,std 为方差 \\right ) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "对数据进行归一化处理\n",
    "定义：把数据经过处理后使之限定在一定的范围内。比如通常限制在区间[0, 1]或者[-1, 1]\n",
    "Z-score归一化\n",
    "'''\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# x_train:[None, 28, 28] -> [None, 784]\n",
    "# 我们np里的数据是int类型，所以我们需要x_train.astype(np.float32)将数据转化成float32\n",
    "# fit_transform 不仅有数据转化为归一化的功能，还有fit（将数据存储下来）的功能.\n",
    "Z_score = lambda d:scaler.fit_transform(d.astype(np.float32).reshape(-1,1)).reshape(-1,28,28)\n",
    "x_train_scaled = Z_score(x_train)\n",
    "x_valid_scaled = Z_score(x_valid)\n",
    "x_test_scaled = Z_score(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tf.Tensor(12.5, shape=(), dtype=float32)\ntf.Tensor(6.75, shape=(), dtype=float32)\ntf.Tensor(6.75, shape=(), dtype=float32)\n"
    }
   ],
   "source": [
    "metric = keras.metrics.MeanSquaredError()\n",
    "print(metric([5.],[2.,1],))\n",
    "metric.reset_states\n",
    "print(metric([0.],[1.]))\n",
    "\n",
    "print(metric.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch : 遍历训练集metric\n",
    "- 自动求导\n",
    "epoch结束:验证集 metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "1718\nEpoch 0 train mse: 8.444045\t valid mse: 8.318773\nEpoch 1 train mse: 8.288684\t valid mse: 8.272807\nEpoch 2 train mse: 8.279286\t valid mse: 8.326979\nEpoch 3 train mse: 8.27741\t valid mse: 8.239223\nEpoch 4 train mse: 8.273457\t valid mse: 8.242294\nEpoch 5 train mse: 8.217403\t valid mse: 8.25629\nEpoch 6 train mse: 8.296442\t valid mse: 8.23847\nEpoch 7 train mse: 8.26509\t valid mse: 8.238471\nEpoch 8 train mse: 8.295274\t valid mse: 8.272746\nEpoch 9 train mse: 8.289257\t valid mse: 8.2716055\n"
    }
   ],
   "source": [
    "epochs = 10\n",
    "batch_size = 32\n",
    "steps_per_epoch = len(x_train_scaled) // batch_size\n",
    "print(steps_per_epoch)\n",
    "optimizer = keras.optimizers.SGD()\n",
    "\n",
    "def random_batch(x,y,batch_size=32):\n",
    "    idx = np.random.randint(0, len(x), size=batch_size)\n",
    "    return x[idx], y[idx]\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation = 'relu', input_shape=x_train.shape[1:]),\n",
    "    keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "metric = keras.metrics.MeanSquaredError()\n",
    "for epoch in range(epochs):\n",
    "    metric.reset_states()\n",
    "    for step in range(steps_per_epoch):\n",
    "        x_batch, y_batch = random_batch(x_train_scaled, y_train, batch_size)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(x_batch)\n",
    "            loss = tf.reduce_mean(keras.losses.mean_squared_error(y_batch, y_pred))\n",
    "            metric(y_batch, y_pred)\n",
    "        grads = tape.gradient(loss, model.variables)\n",
    "        grads_and_vars = zip(grads, model.variables)\n",
    "        optimizer.apply_gradients(grads_and_vars)\n",
    "        print('\\rEpoch', epoch, 'train mse:',metric.result().numpy(), end='')\n",
    "        \n",
    "    y_valid_pred = model(x_valid_scaled)\n",
    "    # 将数据转化成float32格式，不然会报错\n",
    "    y_valid = tf.dtypes.cast(y_valid,tf.float32)\n",
    "    valid_loss = tf.reduce_mean(keras.losses.mean_squared_error(y_valid_pred, y_valid))\n",
    "    print('\\t', 'valid mse:', valid_loss.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型的构建\n",
    "tf.keras.models.sequential()\n",
    "普通的模型构建：\n",
    "```\n",
    "model = keras.models.Sequential([\n",
    "    # 第一层输入成，每一个组数据为[28,28]的二维矩阵，通过keras.layers.Flatten压缩成一维矩阵\n",
    "    keras.layers.Flatten(input_shape=[28,28]),\n",
    "    keras.layers.Dense(300, activation = \"relu\"),\n",
    "    keras.layers.Dense(100, activation = \"relu\"),\n",
    "    keras.layers.Dense(10, activation = \"softmax\"),\n",
    "])\n",
    "```\n",
    "批归一化加激活函数模型构建：\n",
    "\n",
    "```\n",
    "model.add(keras.layers.Flatten(input_shape=[28,28]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100,activation='relu'))\n",
    "    # 批归一化\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    '''\n",
    "    model.add(keras.layers.Dense(100))\n",
    "    # 将批归一化放在激活函数之前\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    # 我们还可以将激活函数设置为一个层次\n",
    "    model.add(keras.layers.Activation('selu'))\n",
    "    model.add(keras.layers.Dense(10,activation='softmax'))\n",
    "    '''\n",
    "```\n",
    "\n",
    "添加dropout层，一般情况下我们只在最后几层添加dropout：\n",
    "\n",
    "```\n",
    "# deep_neural_network模型的构建(20层)\n",
    "model.add(keras.layers.Flatten(input_shape=[28,28]))\n",
    "for _ in range(20):\n",
    "    # 其中selu是一个自带归一化的激活函数,会在\n",
    "    model.add(keras.layers.Dense(100,activation='selu'))\n",
    "# AlphaDropout相比 强大在：1，均值和方差不变 2.归一化的性质不变，分布不会发生变化，可以和批归一化一起使用\n",
    "model.add(keras.layers.AlphaDropout(rate=0.5,))\n",
    "# model.add(keras.layers.Dropout(rate=0.5))\n",
    "model.add(keras.layers.Dense(10,activation='softmax'))\n",
    "```\n",
    "wide&deep模型的构建：\n",
    "```\n",
    "# 使用API实现wide&deep模型\n",
    "class WideDeepModel(keras.models.Model):\n",
    "    def __init__(self):\n",
    "        super(WideDeepModel, self).__init__()\n",
    "        '''定义模型的层次'''\n",
    "        self.hidden1_layer = keras.layers.Dense(30, activation='relu')\n",
    "        self.hidden2_layer = keras.layers.Dense(30, activation='relu')\n",
    "        self.output_layer = keras.layers.Dense(1)\n",
    "        \n",
    "\n",
    "    def call(self, input):\n",
    "        '''完成模型的正向计算'''\n",
    "        hidden1 = self.hidden1_layer(input)\n",
    "        hidden2 = self.hidden2_layer(input)\n",
    "        concat = keras.layers.concatenate([input, hidden2])\n",
    "        output = self.output_layer(concat)\n",
    "        return output\n",
    "# wd模型的建立方式1,这样可以展示更多的细节\n",
    "'''\n",
    "# none 是样本数目，8对应的是features的数目\n",
    "Model: \"wide_deep_model_14\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "dense_42 (Dense)             multiple                  270       \n",
    "_________________________________________________________________\n",
    "dense_43 (Dense)             multiple                  270       \n",
    "_________________________________________________________________\n",
    "dense_44 (Dense)             multiple                  39        \n",
    "=================================================================\n",
    "'''\n",
    "model = WideDeepModel()\n",
    "# wd模型的建立方式2\n",
    "# model = keras.models.Sequential([\n",
    "#     WideDeepModel(),\n",
    "# ])\n",
    "\n",
    "'''\n",
    "# none 是样本数目，8对应的是features的数目\n",
    "Model: \"sequential_1\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "wide_deep_model_13 (WideDeep multiple                  579       \n",
    "=================================================================\n",
    "'''\n",
    "model.build(input_shape=(None, 8))\n",
    "```\n",
    "对于多输入的wide&deep模型的构建：\n",
    "```\n",
    "# 前五个feature当作wide模型的输入\n",
    "input_wide = keras.layers.Input(shape=[5])\n",
    "# 后六个feature当作deep模型的输入\n",
    "input_deep = keras.layers.Input(shape=[6])\n",
    "# deep_model有两个隐含层\n",
    "hidden1 = keras.layers.Dense(30,activation='relu')(input_deep)\n",
    "hidden2 = keras.layers.Dense(30,activation='relu')(hidden1)\n",
    "# 将构建的deep层和wide给拼接起来\n",
    "concat = keras.layers.concatenate([input_wide, hidden2])\n",
    "# 定义输出层\n",
    "output = keras.layers.Dense(1)\n",
    "# 模型定义完成\n",
    "model = keras.model.Model(inputs=[input_wide, input_deep], outputs=[output]))\n",
    "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
    "# 对于deep&and模型多输入的时候，我们的的训练数据也需要进行拆分\n",
    "x_train_scaled_wide = x_train_scaled[:,:5]\n",
    "x_train_scaled_deep = x_train_scaled[:,2:]\n",
    "x_valid_scaled_wide = x_valid_scaled[:,:5]\n",
    "x_valid_scaled_deep = x_valid_scaled[:,2:]\n",
    "x_test_scaled_wide = x_test_scaled[:,:5]\n",
    "x_test_scaled_deep = x_test_scaled[:,2:]\n",
    "# 数据的训练\n",
    "history = model.fit([x_train_scaled_wide,x_train_scaled_deep], \n",
    "                        y_train,\n",
    "                        epochs=10,\n",
    "                        validation_data=([x_valid_scaled_wide,x_valid_scaled_deep],y_valid))\n",
    "# mdhistory.history\n",
    "```\n",
    "对于多输入多输出的wide&deep模型的构建：\n",
    "```\n",
    "# 前五个feature当作wide模型的输入\n",
    "input_wide = keras.layers.Input(shape=[5])\n",
    "# 后六个feature当作deep模型的输入\n",
    "input_deep = keras.layers.Input(shape=[6])\n",
    "# deep_model有两个隐含层\n",
    "hidden1 = keras.layers.Dense(30,activation='relu')(input_deep)\n",
    "hidden2 = keras.layers.Dense(30,activation='relu')(hidden1)\n",
    "# 将构建的deep层和wide给拼接起来\n",
    "concat = keras.layers.concatenate([input_wide, hidden2])\n",
    "# 定义输出层\n",
    "output = keras.layers.Dense(1)(concat)\n",
    "# 在hidden2层后我们定义一次输出\n",
    "output2 = keras.layers.Dense(1)(hidden2)\n",
    "# 模型定义完成\n",
    "model = keras.models.Model(inputs=[input_wide, input_deep], \n",
    "                            outputs=[output,output2])\n",
    "\n",
    "# reason for sparse : y->index, y->one_hot->[],我们需要将y处理成one_hot向量所以用sparse\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",optimizer='sgd', metrics= ['accuracy'])\n",
    "# 定义的模型中的所有层\n",
    "model.layers\n",
    "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
    "# 对于deep&and模型我们的的训练数据也需要进行拆分\n",
    "x_train_scaled_wide = x_train_scaled[:,:5]\n",
    "x_train_scaled_deep = x_train_scaled[:,2:]\n",
    "x_valid_scaled_wide = x_valid_scaled[:,:5]\n",
    "x_valid_scaled_deep = x_valid_scaled[:,2:]\n",
    "x_test_scaled_wide = x_test_scaled[:,:5]\n",
    "x_test_scaled_deep = x_test_scaled[:,2:]\n",
    "# 数据的训练, 当有多个输出的时候，我们的y_train也需要定义两份\n",
    "history = model.fit([x_train_scaled_wide,x_train_scaled_deep], \n",
    "                        [y_train,y_train],\n",
    "                        epochs=10,\n",
    "                        validation_data=([x_valid_scaled_wide,x_valid_scaled_deep],[y_valid,y_valid]))\n",
    "# mdhistory.history\n",
    "```\n",
    "超参：\n",
    "神经网络有很多训练过程中不变的参数\n",
    "- 网络结构参数：几层，每层宽度，每层激活函数等\n",
    "- 训练参数：batch_size，学习率，学习率衰减算法等\n",
    "手工去试的话比较耗费人力\n",
    "超参搜索策略：\n",
    "- 网络搜索\n",
    "- 随机搜索\n",
    "- 遗传算法搜索\n",
    "- 启发式搜索\n",
    "使用sklearn封装keras模型：\n",
    "```\n",
    "'''\n",
    "RandomizedSearchCV\n",
    "转化为skleran的model\n",
    "定义参数集合\n",
    "搜索参数\n",
    "'''\n",
    "def build_model(hidden_layers = 1,\n",
    "                            layer_size = 30,\n",
    "                            learning_rate = 3e-3):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Dense(layer_size, activation='relu',inpute_shape=x_train.shape[1:]))\n",
    "    for _ in range(hidden_layers - 1):\n",
    "        model.add(keras.layers.Dense(layer_size,activation = 'relu'))\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    optimizer = keras.optimizers.SGD(leraning_rate)\n",
    "    mode.compile(loss='mse',optimizer=optimizer)\n",
    "    return model\n",
    "skleran_model = keras.wrappers.scikit_learn.KerasRegressor(build_model)\n",
    "sklearn_model.fit(x_train_scaled, y_train, epochs = 100, validation_data = (x_valid_scaled,y_valid))\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation='relu'),\n",
    "    keras.layers.Dense(1),\n",
    "])\n",
    "```\n",
    "自定义层的两种方式\n",
    "```\n",
    "# 自定义层的两种方式\n",
    "Customized_softplus = keras.layers.Lambda(lambda x:tf.nn.softplus(x))\n",
    "\n",
    "class CustomizedDenseLayer(keras.layers.Layer):\n",
    "    def __init__(self, units, activation=None, **kwargs):\n",
    "        # 层次的输出：units\n",
    "        self.units = units\n",
    "        self.activation = keras.layers.Activation(activation)\n",
    "        super(CustomizedDenseLayer, self).__init__(**kwargs)\n",
    "    def build(self, input_shape):\n",
    "        '''构建所需要的参数'''\n",
    "        # x*w+b input_shape:[none,a], w;[a,b],output_shape:[none,b]\n",
    "        self.kernel = self.add_weight(name='kernel', shape=(input_shape[1],self.units),initializer='uniform',trainable=True)\n",
    "        self.bias = self.add_weight(name='bias', shape=(self.units,),initializer='zeros',trainable=True)\n",
    "        super(CustomizedDenseLayer, self).build(input_shape)\n",
    "    def call(self,x):\n",
    "        '''完整计算向量'''\n",
    "        return self.activation(x @ self.kernel + self.bias)\n",
    "model = keras.models.Sequential([\n",
    "    CustomizedDenseLayer(30, activation='relu'),\n",
    "    CustomizedDenseLayer(1),\n",
    "    # Customized_softplus 和 keras.layers.Dense(1,activation='softplus')是等价的\n",
    "    Customized_softplus,\n",
    "])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "训练集中第0个数据： (28, 28)\nModel: \"sequential_9\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nflatten_2 (Flatten)          (None, 784)               0         \n_________________________________________________________________\ndense_39 (Dense)             (None, 300)               235500    \n_________________________________________________________________\ndense_40 (Dense)             (None, 100)               30100     \n_________________________________________________________________\ndense_41 (Dense)             (None, 10)                1010      \n=================================================================\nTotal params: 266,610\nTrainable params: 266,610\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "# 模型的构建\n",
    "# tf.keras.models.sequential()\n",
    "# 将 28*28 的矩阵展开为一维向量\n",
    "print('训练集中第0个数据：',x_train[0].shape)\n",
    "model = keras.models.Sequential([\n",
    "    # 第一层输入成，每一个组数据为[28,28]的二维矩阵，通过keras.layers.Flatten压缩成一维矩阵\n",
    "    keras.layers.Flatten(input_shape=[28,28]),\n",
    "    keras.layers.Dense(300, activation = \"relu\"),\n",
    "    keras.layers.Dense(100, activation = \"relu\"),\n",
    "    keras.layers.Dense(10, activation = \"softmax\"),\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",optimizer='sgd', metrics= ['accuracy'])\n",
    "# 定义的模型中的所有层\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 超参数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对于deep&wide模型我们的的训练数据也需要进行拆分---------是哦那个deep&wide模型得时候使用\n",
    "x_train_scaled_wide = x_train_scaled[:,:5]\n",
    "x_train_scaled_deep = x_train_scaled[:,2:]\n",
    "x_valid_scaled_wide = x_valid_scaled[:,:5]\n",
    "x_valid_scaled_deep = x_valid_scaled[:,2:]\n",
    "x_test_scaled_wide = x_test_scaled[:,:5]\n",
    "x_test_scaled_deep = x_test_scaled[:,2:]\n",
    "# 数据的训练, 当有多个输出的时候，我们的y_train也需要定义两份\n",
    "mdhistory = model.fit([x_train_scaled_wide,x_train_scaled_deep], \n",
    "                        [y_train,y_train],\n",
    "                        epochs=10,\n",
    "                        validation_data=([x_valid_scaled_wide,x_valid_scaled_deep],[y_valid,y_valid]))\n",
    "mdhistory.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据的训练\n",
    "### [添加回调函数pg](https://tensorflow.google.cn/api_docs/python/tf/keras/callbacks?hl=zh_cn)\n",
    "- Tensorboard: \n",
    "    - Metrics summary plots 指标摘要图\n",
    "    - Training graph visualization 训练图可视化\n",
    "    - Activation histograms 激活直方图\n",
    "    - Sampled profiling 采样分析\n",
    "- EarlyStopping \n",
    "    - 关注某个指标，比如超参\n",
    "        超参数之一是定型周期（epoch）的数量：亦即应当完整遍历数据集多少次（一次为一个epoch）？如果epoch数量太少，网络有可能发生欠拟合（即对于定型数据的学习不够充分）；如果epoch数量太多，则有可能发生过拟合（即网络对定型数据中的“噪声”而非信号拟合）。\n",
    "\n",
    "    早停法旨在解决epoch数量需要手动设置的问题。它也可以被视为一种能够避免网络发生过拟合的正则化方法（与L1/L2权重衰减和丢弃法类似）。\n",
    "\n",
    "    根本原因就是因为继续训练会导致测试集上的准确率下降。\n",
    "    那继续训练导致测试准确率下降的原因猜测可能是1. 过拟合 2. 学习率过大导致不收敛\n",
    "\n",
    "### 对layers进行数据训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Train on 55000 samples, validate on 5000 samples\nEpoch 1/10\n55000/55000 [==============================] - 5s 93us/sample - loss: inf - accuracy: 0.0982 - val_loss: 2.3041 - val_accuracy: 0.0984\nEpoch 2/10\n55000/55000 [==============================] - 5s 96us/sample - loss: 2.3027 - accuracy: 0.0982 - val_loss: 2.3040 - val_accuracy: 0.0986\nEpoch 3/10\n55000/55000 [==============================] - 5s 86us/sample - loss: 2.3027 - accuracy: 0.1006 - val_loss: 2.3041 - val_accuracy: 0.0986\nEpoch 4/10\n55000/55000 [==============================] - 4s 81us/sample - loss: 2.3027 - accuracy: 0.0987 - val_loss: 2.3042 - val_accuracy: 0.0914\nEpoch 5/10\n55000/55000 [==============================] - 7s 121us/sample - loss: 2.3027 - accuracy: 0.0984 - val_loss: 2.3040 - val_accuracy: 0.0980\nEpoch 6/10\n55000/55000 [==============================] - 11s 194us/sample - loss: 2.3027 - accuracy: 0.0972 - val_loss: 2.3039 - val_accuracy: 0.0914\nEpoch 7/10\n55000/55000 [==============================] - 15s 267us/sample - loss: 2.3027 - accuracy: 0.0989 - val_loss: 2.3042 - val_accuracy: 0.0976\nEpoch 8/10\n55000/55000 [==============================] - 15s 281us/sample - loss: 2.3027 - accuracy: 0.0985 - val_loss: 2.3041 - val_accuracy: 0.0914\nEpoch 9/10\n55000/55000 [==============================] - 15s 271us/sample - loss: 2.3027 - accuracy: 0.0978 - val_loss: 2.3042 - val_accuracy: 0.0914\nEpoch 10/10\n55000/55000 [==============================] - 11s 197us/sample - loss: 2.3027 - accuracy: 0.1002 - val_loss: 2.3041 - val_accuracy: 0.0914\n"
    },
    {
     "data": {
      "text/plain": "{'loss': [inf,\n  2.3027012555209074,\n  2.302684170844338,\n  2.302700057740645,\n  2.3026898735739967,\n  2.3026884390050713,\n  2.3026932554765183,\n  2.302697267150879,\n  2.3026952650243584,\n  2.3026737274516713],\n 'accuracy': [0.09821818,\n  0.09821818,\n  0.10056364,\n  0.09872727,\n  0.0984,\n  0.09716364,\n  0.09885454,\n  0.098527275,\n  0.097763635,\n  0.10023636],\n 'val_loss': [2.3040645393371584,\n  2.3040028648376465,\n  2.304090444946289,\n  2.304177123260498,\n  2.3040085578918457,\n  2.303938995361328,\n  2.304162268447876,\n  2.3041053184509277,\n  2.304207458496094,\n  2.304115872955322],\n 'val_accuracy': [0.0984,\n  0.0986,\n  0.0986,\n  0.0914,\n  0.098,\n  0.0914,\n  0.0976,\n  0.0914,\n  0.0914,\n  0.0914]}"
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logdir = os.path.join('callbacks')\n",
    "\n",
    "if not os.path.exists(logdir):\n",
    "    os.mkdir(logdir)\n",
    "output_model_file = os.path.join(logdir,\n",
    "                                 \"fashion_mnist_model.h5\")\n",
    "callbacks = [\n",
    "    keras.callbacks.TensorBoard(logdir),\n",
    "    keras.callbacks.ModelCheckpoint(output_model_file, save_best_only = True),\n",
    "    # 由于epochs设置的比较小，可能不会触发，可以将epochs调大点，看看EarlyStopping的运行情况\n",
    "    keras.callbacks.EarlyStopping(patience=5, min_delta=1e-3),\n",
    "]\n",
    "\n",
    "# 数据的训练\n",
    "history = model.fit(x_train, y_train, epochs=10,validation_data=(x_valid,y_valid))\n",
    "history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## matloplib可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matloplib可视化\n",
    "def plot_learning_cruves(history):\n",
    "    pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "    plt.grid(True)\n",
    "    plt.gca().set_ylim(0,5)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"306.677344pt\" version=\"1.1\" viewBox=\"0 0 474.1625 306.677344\" width=\"474.1625pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <defs>\r\n  <style type=\"text/css\">\r\n*{stroke-linecap:butt;stroke-linejoin:round;}\r\n  </style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 306.677344 \r\nL 474.1625 306.677344 \r\nL 474.1625 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 20.5625 282.799219 \r\nL 466.9625 282.799219 \r\nL 466.9625 10.999219 \r\nL 20.5625 10.999219 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <path clip-path=\"url(#peead855756)\" d=\"M 40.853409 282.799219 \r\nL 40.853409 10.999219 \r\n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_2\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m9e801ce10e\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.853409\" xlink:href=\"#m9e801ce10e\" y=\"282.799219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n      </defs>\r\n      <g transform=\"translate(37.672159 297.397656)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_3\">\r\n      <path clip-path=\"url(#peead855756)\" d=\"M 131.035227 282.799219 \r\nL 131.035227 10.999219 \r\n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"131.035227\" xlink:href=\"#m9e801ce10e\" y=\"282.799219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 2 -->\r\n      <defs>\r\n       <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n      </defs>\r\n      <g transform=\"translate(127.853977 297.397656)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_5\">\r\n      <path clip-path=\"url(#peead855756)\" d=\"M 221.217045 282.799219 \r\nL 221.217045 10.999219 \r\n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"221.217045\" xlink:href=\"#m9e801ce10e\" y=\"282.799219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 4 -->\r\n      <defs>\r\n       <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n      </defs>\r\n      <g transform=\"translate(218.035795 297.397656)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_7\">\r\n      <path clip-path=\"url(#peead855756)\" d=\"M 311.398864 282.799219 \r\nL 311.398864 10.999219 \r\n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"311.398864\" xlink:href=\"#m9e801ce10e\" y=\"282.799219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 6 -->\r\n      <defs>\r\n       <path d=\"M 33.015625 40.375 \r\nQ 26.375 40.375 22.484375 35.828125 \r\nQ 18.609375 31.296875 18.609375 23.390625 \r\nQ 18.609375 15.53125 22.484375 10.953125 \r\nQ 26.375 6.390625 33.015625 6.390625 \r\nQ 39.65625 6.390625 43.53125 10.953125 \r\nQ 47.40625 15.53125 47.40625 23.390625 \r\nQ 47.40625 31.296875 43.53125 35.828125 \r\nQ 39.65625 40.375 33.015625 40.375 \r\nz\r\nM 52.59375 71.296875 \r\nL 52.59375 62.3125 \r\nQ 48.875 64.0625 45.09375 64.984375 \r\nQ 41.3125 65.921875 37.59375 65.921875 \r\nQ 27.828125 65.921875 22.671875 59.328125 \r\nQ 17.53125 52.734375 16.796875 39.40625 \r\nQ 19.671875 43.65625 24.015625 45.921875 \r\nQ 28.375 48.1875 33.59375 48.1875 \r\nQ 44.578125 48.1875 50.953125 41.515625 \r\nQ 57.328125 34.859375 57.328125 23.390625 \r\nQ 57.328125 12.15625 50.6875 5.359375 \r\nQ 44.046875 -1.421875 33.015625 -1.421875 \r\nQ 20.359375 -1.421875 13.671875 8.265625 \r\nQ 6.984375 17.96875 6.984375 36.375 \r\nQ 6.984375 53.65625 15.1875 63.9375 \r\nQ 23.390625 74.21875 37.203125 74.21875 \r\nQ 40.921875 74.21875 44.703125 73.484375 \r\nQ 48.484375 72.75 52.59375 71.296875 \r\nz\r\n\" id=\"DejaVuSans-54\"/>\r\n      </defs>\r\n      <g transform=\"translate(308.217614 297.397656)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-54\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_9\">\r\n      <path clip-path=\"url(#peead855756)\" d=\"M 401.580682 282.799219 \r\nL 401.580682 10.999219 \r\n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"401.580682\" xlink:href=\"#m9e801ce10e\" y=\"282.799219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 8 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 34.625 \r\nQ 24.75 34.625 20.71875 30.859375 \r\nQ 16.703125 27.09375 16.703125 20.515625 \r\nQ 16.703125 13.921875 20.71875 10.15625 \r\nQ 24.75 6.390625 31.78125 6.390625 \r\nQ 38.8125 6.390625 42.859375 10.171875 \r\nQ 46.921875 13.96875 46.921875 20.515625 \r\nQ 46.921875 27.09375 42.890625 30.859375 \r\nQ 38.875 34.625 31.78125 34.625 \r\nz\r\nM 21.921875 38.8125 \r\nQ 15.578125 40.375 12.03125 44.71875 \r\nQ 8.5 49.078125 8.5 55.328125 \r\nQ 8.5 64.0625 14.71875 69.140625 \r\nQ 20.953125 74.21875 31.78125 74.21875 \r\nQ 42.671875 74.21875 48.875 69.140625 \r\nQ 55.078125 64.0625 55.078125 55.328125 \r\nQ 55.078125 49.078125 51.53125 44.71875 \r\nQ 48 40.375 41.703125 38.8125 \r\nQ 48.828125 37.15625 52.796875 32.3125 \r\nQ 56.78125 27.484375 56.78125 20.515625 \r\nQ 56.78125 9.90625 50.3125 4.234375 \r\nQ 43.84375 -1.421875 31.78125 -1.421875 \r\nQ 19.734375 -1.421875 13.25 4.234375 \r\nQ 6.78125 9.90625 6.78125 20.515625 \r\nQ 6.78125 27.484375 10.78125 32.3125 \r\nQ 14.796875 37.15625 21.921875 38.8125 \r\nz\r\nM 18.3125 54.390625 \r\nQ 18.3125 48.734375 21.84375 45.5625 \r\nQ 25.390625 42.390625 31.78125 42.390625 \r\nQ 38.140625 42.390625 41.71875 45.5625 \r\nQ 45.3125 48.734375 45.3125 54.390625 \r\nQ 45.3125 60.0625 41.71875 63.234375 \r\nQ 38.140625 66.40625 31.78125 66.40625 \r\nQ 25.390625 66.40625 21.84375 63.234375 \r\nQ 18.3125 60.0625 18.3125 54.390625 \r\nz\r\n\" id=\"DejaVuSans-56\"/>\r\n      </defs>\r\n      <g transform=\"translate(398.399432 297.397656)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-56\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_11\">\r\n      <path clip-path=\"url(#peead855756)\" d=\"M 20.5625 282.799219 \r\nL 466.9625 282.799219 \r\n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_12\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"ma82dfdbbf2\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#ma82dfdbbf2\" y=\"282.799219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(7.2 286.598437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_13\">\r\n      <path clip-path=\"url(#peead855756)\" d=\"M 20.5625 228.439219 \r\nL 466.9625 228.439219 \r\n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_14\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#ma82dfdbbf2\" y=\"228.439219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 1 -->\r\n      <defs>\r\n       <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n      </defs>\r\n      <g transform=\"translate(7.2 232.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_15\">\r\n      <path clip-path=\"url(#peead855756)\" d=\"M 20.5625 174.079219 \r\nL 466.9625 174.079219 \r\n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_16\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#ma82dfdbbf2\" y=\"174.079219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 2 -->\r\n      <g transform=\"translate(7.2 177.878437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_17\">\r\n      <path clip-path=\"url(#peead855756)\" d=\"M 20.5625 119.719219 \r\nL 466.9625 119.719219 \r\n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_18\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#ma82dfdbbf2\" y=\"119.719219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 3 -->\r\n      <defs>\r\n       <path d=\"M 40.578125 39.3125 \r\nQ 47.65625 37.796875 51.625 33 \r\nQ 55.609375 28.21875 55.609375 21.1875 \r\nQ 55.609375 10.40625 48.1875 4.484375 \r\nQ 40.765625 -1.421875 27.09375 -1.421875 \r\nQ 22.515625 -1.421875 17.65625 -0.515625 \r\nQ 12.796875 0.390625 7.625 2.203125 \r\nL 7.625 11.71875 \r\nQ 11.71875 9.328125 16.59375 8.109375 \r\nQ 21.484375 6.890625 26.8125 6.890625 \r\nQ 36.078125 6.890625 40.9375 10.546875 \r\nQ 45.796875 14.203125 45.796875 21.1875 \r\nQ 45.796875 27.640625 41.28125 31.265625 \r\nQ 36.765625 34.90625 28.71875 34.90625 \r\nL 20.21875 34.90625 \r\nL 20.21875 43.015625 \r\nL 29.109375 43.015625 \r\nQ 36.375 43.015625 40.234375 45.921875 \r\nQ 44.09375 48.828125 44.09375 54.296875 \r\nQ 44.09375 59.90625 40.109375 62.90625 \r\nQ 36.140625 65.921875 28.71875 65.921875 \r\nQ 24.65625 65.921875 20.015625 65.03125 \r\nQ 15.375 64.15625 9.8125 62.3125 \r\nL 9.8125 71.09375 \r\nQ 15.4375 72.65625 20.34375 73.4375 \r\nQ 25.25 74.21875 29.59375 74.21875 \r\nQ 40.828125 74.21875 47.359375 69.109375 \r\nQ 53.90625 64.015625 53.90625 55.328125 \r\nQ 53.90625 49.265625 50.4375 45.09375 \r\nQ 46.96875 40.921875 40.578125 39.3125 \r\nz\r\n\" id=\"DejaVuSans-51\"/>\r\n      </defs>\r\n      <g transform=\"translate(7.2 123.518437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_19\">\r\n      <path clip-path=\"url(#peead855756)\" d=\"M 20.5625 65.359219 \r\nL 466.9625 65.359219 \r\n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_20\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#ma82dfdbbf2\" y=\"65.359219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 4 -->\r\n      <g transform=\"translate(7.2 69.158437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_21\">\r\n      <path clip-path=\"url(#peead855756)\" d=\"M 20.5625 10.999219 \r\nL 466.9625 10.999219 \r\n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_22\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#ma82dfdbbf2\" y=\"10.999219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 5 -->\r\n      <defs>\r\n       <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n      </defs>\r\n      <g transform=\"translate(7.2 14.798437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_23\">\r\n    <path clip-path=\"url(#peead855756)\" d=\"M 85.944318 157.624378 \r\nL 131.035227 157.625307 \r\nL 176.126136 157.624444 \r\nL 221.217045 157.624997 \r\nL 266.307955 157.625075 \r\nL 311.398864 157.624813 \r\nL 356.489773 157.624595 \r\nL 401.580682 157.624704 \r\nL 446.671591 157.625875 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"line2d_24\">\r\n    <path clip-path=\"url(#peead855756)\" d=\"M 40.853409 277.460078 \r\nL 85.944318 277.460078 \r\nL 131.035227 277.332579 \r\nL 176.126136 277.432404 \r\nL 221.217045 277.450195 \r\nL 266.307955 277.517403 \r\nL 311.398864 277.425486 \r\nL 356.489773 277.443276 \r\nL 401.580682 277.484788 \r\nL 446.671591 277.35037 \r\n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"line2d_25\">\r\n    <path clip-path=\"url(#peead855756)\" d=\"M 40.853409 157.55027 \r\nL 85.944318 157.553623 \r\nL 131.035227 157.548862 \r\nL 176.126136 157.54415 \r\nL 221.217045 157.553314 \r\nL 266.307955 157.557095 \r\nL 311.398864 157.544958 \r\nL 356.489773 157.548054 \r\nL 401.580682 157.542501 \r\nL 446.671591 157.54748 \r\n\" style=\"fill:none;stroke:#2ca02c;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"line2d_26\">\r\n    <path clip-path=\"url(#peead855756)\" d=\"M 40.853409 277.450195 \r\nL 85.944318 277.439323 \r\nL 131.035227 277.439323 \r\nL 176.126136 277.830715 \r\nL 221.217045 277.471939 \r\nL 266.307955 277.830715 \r\nL 311.398864 277.493683 \r\nL 356.489773 277.830715 \r\nL 401.580682 277.830715 \r\nL 446.671591 277.830715 \r\n\" style=\"fill:none;stroke:#d62728;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 20.5625 282.799219 \r\nL 20.5625 10.999219 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 466.9625 282.799219 \r\nL 466.9625 10.999219 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 20.5625 282.799219 \r\nL 466.9625 282.799219 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 20.5625 10.999219 \r\nL 466.9625 10.999219 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"legend_1\">\r\n    <g id=\"patch_7\">\r\n     <path d=\"M 363.01875 78.267969 \r\nL 459.9625 78.267969 \r\nQ 461.9625 78.267969 461.9625 76.267969 \r\nL 461.9625 17.999219 \r\nQ 461.9625 15.999219 459.9625 15.999219 \r\nL 363.01875 15.999219 \r\nQ 361.01875 15.999219 361.01875 17.999219 \r\nL 361.01875 76.267969 \r\nQ 361.01875 78.267969 363.01875 78.267969 \r\nz\r\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\r\n    </g>\r\n    <g id=\"line2d_27\">\r\n     <path d=\"M 365.01875 24.097656 \r\nL 385.01875 24.097656 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_28\"/>\r\n    <g id=\"text_12\">\r\n     <!-- loss -->\r\n     <defs>\r\n      <path d=\"M 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\n\" id=\"DejaVuSans-108\"/>\r\n      <path d=\"M 30.609375 48.390625 \r\nQ 23.390625 48.390625 19.1875 42.75 \r\nQ 14.984375 37.109375 14.984375 27.296875 \r\nQ 14.984375 17.484375 19.15625 11.84375 \r\nQ 23.34375 6.203125 30.609375 6.203125 \r\nQ 37.796875 6.203125 41.984375 11.859375 \r\nQ 46.1875 17.53125 46.1875 27.296875 \r\nQ 46.1875 37.015625 41.984375 42.703125 \r\nQ 37.796875 48.390625 30.609375 48.390625 \r\nz\r\nM 30.609375 56 \r\nQ 42.328125 56 49.015625 48.375 \r\nQ 55.71875 40.765625 55.71875 27.296875 \r\nQ 55.71875 13.875 49.015625 6.21875 \r\nQ 42.328125 -1.421875 30.609375 -1.421875 \r\nQ 18.84375 -1.421875 12.171875 6.21875 \r\nQ 5.515625 13.875 5.515625 27.296875 \r\nQ 5.515625 40.765625 12.171875 48.375 \r\nQ 18.84375 56 30.609375 56 \r\nz\r\n\" id=\"DejaVuSans-111\"/>\r\n      <path d=\"M 44.28125 53.078125 \r\nL 44.28125 44.578125 \r\nQ 40.484375 46.53125 36.375 47.5 \r\nQ 32.28125 48.484375 27.875 48.484375 \r\nQ 21.1875 48.484375 17.84375 46.4375 \r\nQ 14.5 44.390625 14.5 40.28125 \r\nQ 14.5 37.15625 16.890625 35.375 \r\nQ 19.28125 33.59375 26.515625 31.984375 \r\nL 29.59375 31.296875 \r\nQ 39.15625 29.25 43.1875 25.515625 \r\nQ 47.21875 21.78125 47.21875 15.09375 \r\nQ 47.21875 7.46875 41.1875 3.015625 \r\nQ 35.15625 -1.421875 24.609375 -1.421875 \r\nQ 20.21875 -1.421875 15.453125 -0.5625 \r\nQ 10.6875 0.296875 5.421875 2 \r\nL 5.421875 11.28125 \r\nQ 10.40625 8.6875 15.234375 7.390625 \r\nQ 20.0625 6.109375 24.8125 6.109375 \r\nQ 31.15625 6.109375 34.5625 8.28125 \r\nQ 37.984375 10.453125 37.984375 14.40625 \r\nQ 37.984375 18.0625 35.515625 20.015625 \r\nQ 33.0625 21.96875 24.703125 23.78125 \r\nL 21.578125 24.515625 \r\nQ 13.234375 26.265625 9.515625 29.90625 \r\nQ 5.8125 33.546875 5.8125 39.890625 \r\nQ 5.8125 47.609375 11.28125 51.796875 \r\nQ 16.75 56 26.8125 56 \r\nQ 31.78125 56 36.171875 55.265625 \r\nQ 40.578125 54.546875 44.28125 53.078125 \r\nz\r\n\" id=\"DejaVuSans-115\"/>\r\n     </defs>\r\n     <g transform=\"translate(393.01875 27.597656)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"88.964844\" xlink:href=\"#DejaVuSans-115\"/>\r\n      <use x=\"141.064453\" xlink:href=\"#DejaVuSans-115\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"line2d_29\">\r\n     <path d=\"M 365.01875 38.775781 \r\nL 385.01875 38.775781 \r\n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_30\"/>\r\n    <g id=\"text_13\">\r\n     <!-- accuracy -->\r\n     <defs>\r\n      <path d=\"M 34.28125 27.484375 \r\nQ 23.390625 27.484375 19.1875 25 \r\nQ 14.984375 22.515625 14.984375 16.5 \r\nQ 14.984375 11.71875 18.140625 8.90625 \r\nQ 21.296875 6.109375 26.703125 6.109375 \r\nQ 34.1875 6.109375 38.703125 11.40625 \r\nQ 43.21875 16.703125 43.21875 25.484375 \r\nL 43.21875 27.484375 \r\nz\r\nM 52.203125 31.203125 \r\nL 52.203125 0 \r\nL 43.21875 0 \r\nL 43.21875 8.296875 \r\nQ 40.140625 3.328125 35.546875 0.953125 \r\nQ 30.953125 -1.421875 24.3125 -1.421875 \r\nQ 15.921875 -1.421875 10.953125 3.296875 \r\nQ 6 8.015625 6 15.921875 \r\nQ 6 25.140625 12.171875 29.828125 \r\nQ 18.359375 34.515625 30.609375 34.515625 \r\nL 43.21875 34.515625 \r\nL 43.21875 35.40625 \r\nQ 43.21875 41.609375 39.140625 45 \r\nQ 35.0625 48.390625 27.6875 48.390625 \r\nQ 23 48.390625 18.546875 47.265625 \r\nQ 14.109375 46.140625 10.015625 43.890625 \r\nL 10.015625 52.203125 \r\nQ 14.9375 54.109375 19.578125 55.046875 \r\nQ 24.21875 56 28.609375 56 \r\nQ 40.484375 56 46.34375 49.84375 \r\nQ 52.203125 43.703125 52.203125 31.203125 \r\nz\r\n\" id=\"DejaVuSans-97\"/>\r\n      <path d=\"M 48.78125 52.59375 \r\nL 48.78125 44.1875 \r\nQ 44.96875 46.296875 41.140625 47.34375 \r\nQ 37.3125 48.390625 33.40625 48.390625 \r\nQ 24.65625 48.390625 19.8125 42.84375 \r\nQ 14.984375 37.3125 14.984375 27.296875 \r\nQ 14.984375 17.28125 19.8125 11.734375 \r\nQ 24.65625 6.203125 33.40625 6.203125 \r\nQ 37.3125 6.203125 41.140625 7.25 \r\nQ 44.96875 8.296875 48.78125 10.40625 \r\nL 48.78125 2.09375 \r\nQ 45.015625 0.34375 40.984375 -0.53125 \r\nQ 36.96875 -1.421875 32.421875 -1.421875 \r\nQ 20.0625 -1.421875 12.78125 6.34375 \r\nQ 5.515625 14.109375 5.515625 27.296875 \r\nQ 5.515625 40.671875 12.859375 48.328125 \r\nQ 20.21875 56 33.015625 56 \r\nQ 37.15625 56 41.109375 55.140625 \r\nQ 45.0625 54.296875 48.78125 52.59375 \r\nz\r\n\" id=\"DejaVuSans-99\"/>\r\n      <path d=\"M 8.5 21.578125 \r\nL 8.5 54.6875 \r\nL 17.484375 54.6875 \r\nL 17.484375 21.921875 \r\nQ 17.484375 14.15625 20.5 10.265625 \r\nQ 23.53125 6.390625 29.59375 6.390625 \r\nQ 36.859375 6.390625 41.078125 11.03125 \r\nQ 45.3125 15.671875 45.3125 23.6875 \r\nL 45.3125 54.6875 \r\nL 54.296875 54.6875 \r\nL 54.296875 0 \r\nL 45.3125 0 \r\nL 45.3125 8.40625 \r\nQ 42.046875 3.421875 37.71875 1 \r\nQ 33.40625 -1.421875 27.6875 -1.421875 \r\nQ 18.265625 -1.421875 13.375 4.4375 \r\nQ 8.5 10.296875 8.5 21.578125 \r\nz\r\nM 31.109375 56 \r\nz\r\n\" id=\"DejaVuSans-117\"/>\r\n      <path d=\"M 41.109375 46.296875 \r\nQ 39.59375 47.171875 37.8125 47.578125 \r\nQ 36.03125 48 33.890625 48 \r\nQ 26.265625 48 22.1875 43.046875 \r\nQ 18.109375 38.09375 18.109375 28.8125 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 20.953125 51.171875 25.484375 53.578125 \r\nQ 30.03125 56 36.53125 56 \r\nQ 37.453125 56 38.578125 55.875 \r\nQ 39.703125 55.765625 41.0625 55.515625 \r\nz\r\n\" id=\"DejaVuSans-114\"/>\r\n      <path d=\"M 32.171875 -5.078125 \r\nQ 28.375 -14.84375 24.75 -17.8125 \r\nQ 21.140625 -20.796875 15.09375 -20.796875 \r\nL 7.90625 -20.796875 \r\nL 7.90625 -13.28125 \r\nL 13.1875 -13.28125 \r\nQ 16.890625 -13.28125 18.9375 -11.515625 \r\nQ 21 -9.765625 23.484375 -3.21875 \r\nL 25.09375 0.875 \r\nL 2.984375 54.6875 \r\nL 12.5 54.6875 \r\nL 29.59375 11.921875 \r\nL 46.6875 54.6875 \r\nL 56.203125 54.6875 \r\nz\r\n\" id=\"DejaVuSans-121\"/>\r\n     </defs>\r\n     <g transform=\"translate(393.01875 42.275781)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"61.279297\" xlink:href=\"#DejaVuSans-99\"/>\r\n      <use x=\"116.259766\" xlink:href=\"#DejaVuSans-99\"/>\r\n      <use x=\"171.240234\" xlink:href=\"#DejaVuSans-117\"/>\r\n      <use x=\"234.619141\" xlink:href=\"#DejaVuSans-114\"/>\r\n      <use x=\"275.732422\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"337.011719\" xlink:href=\"#DejaVuSans-99\"/>\r\n      <use x=\"391.992188\" xlink:href=\"#DejaVuSans-121\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"line2d_31\">\r\n     <path d=\"M 365.01875 53.453906 \r\nL 385.01875 53.453906 \r\n\" style=\"fill:none;stroke:#2ca02c;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_32\"/>\r\n    <g id=\"text_14\">\r\n     <!-- val_loss -->\r\n     <defs>\r\n      <path d=\"M 2.984375 54.6875 \r\nL 12.5 54.6875 \r\nL 29.59375 8.796875 \r\nL 46.6875 54.6875 \r\nL 56.203125 54.6875 \r\nL 35.6875 0 \r\nL 23.484375 0 \r\nz\r\n\" id=\"DejaVuSans-118\"/>\r\n      <path d=\"M 50.984375 -16.609375 \r\nL 50.984375 -23.578125 \r\nL -0.984375 -23.578125 \r\nL -0.984375 -16.609375 \r\nz\r\n\" id=\"DejaVuSans-95\"/>\r\n     </defs>\r\n     <g transform=\"translate(393.01875 56.953906)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-118\"/>\r\n      <use x=\"59.179688\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"120.458984\" xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"148.242188\" xlink:href=\"#DejaVuSans-95\"/>\r\n      <use x=\"198.242188\" xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"226.025391\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"287.207031\" xlink:href=\"#DejaVuSans-115\"/>\r\n      <use x=\"339.306641\" xlink:href=\"#DejaVuSans-115\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"line2d_33\">\r\n     <path d=\"M 365.01875 68.410156 \r\nL 385.01875 68.410156 \r\n\" style=\"fill:none;stroke:#d62728;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_34\"/>\r\n    <g id=\"text_15\">\r\n     <!-- val_accuracy -->\r\n     <g transform=\"translate(393.01875 71.910156)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-118\"/>\r\n      <use x=\"59.179688\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"120.458984\" xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"148.242188\" xlink:href=\"#DejaVuSans-95\"/>\r\n      <use x=\"198.242188\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"259.521484\" xlink:href=\"#DejaVuSans-99\"/>\r\n      <use x=\"314.501953\" xlink:href=\"#DejaVuSans-99\"/>\r\n      <use x=\"369.482422\" xlink:href=\"#DejaVuSans-117\"/>\r\n      <use x=\"432.861328\" xlink:href=\"#DejaVuSans-114\"/>\r\n      <use x=\"473.974609\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"535.253906\" xlink:href=\"#DejaVuSans-99\"/>\r\n      <use x=\"590.234375\" xlink:href=\"#DejaVuSans-121\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"peead855756\">\r\n   <rect height=\"271.8\" width=\"446.4\" x=\"20.5625\" y=\"10.999219\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "text/plain": "<Figure size 576x360 with 1 Axes>"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plot_learning_cruves(history)\n",
    "\n",
    "# bug: 为何loss和精确值一直不变？？？？？？？？？？？？？？？？？？？\n",
    "# 没解决"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "### 关于梯度消失问题\n",
    "深度模型中可能出现的问题：参数众多，倒只训练不充分\n",
    "梯度消失问题：首先梯度下降是指，一个数按照其此点最大导数的反方向更新。\n",
    "对于一个多层次的神经网络来说，比目标函数比较远的但是梯度比较微小的现象。\n",
    "什么情况会导致？ 一般发生在深度模型中，根据链式法则：符合函数{f(g(x))}\n",
    "梯度下降的时候我们需要对每一个嵌套的复合函数进行求导再相乘，最后如果求出来的导数小于1，多此相乘就会导致梯度消失。\n",
    "1.01^99=37.8\n",
    "0.99^99=0.03\n",
    "\n",
    "批归一化可以在一定程度上缓解梯度消失~\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "10000/10000 [==============================] - 2s 198us/sample - loss: 2.3026 - accuracy: 0.1000\n"
    },
    {
     "data": {
      "text/plain": "[2.3026404083251952, 0.1]"
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.6.7 64-bit ('rec_env': virtualenv)",
   "language": "python",
   "name": "python36764bitrecenvvirtualenvc590a763a4564b6f98e8e2c90348aa96"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7-final"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}